{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from OpenEyesClassificator import OpenEyesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class OpenEyesClassificator(nn.Module):\n",
    "#     def __init__(self, dropout_rate=0.6):\n",
    "#         super(OpenEyesClassificator, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
    "#         self.bn3 = nn.BatchNorm1d(128)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(128, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(self.relu1(self.bn1(self.conv1(x))))\n",
    "#         x = self.dropout1(x)\n",
    "        \n",
    "#         x = self.pool(self.relu2(self.bn2(self.conv2(x))))\n",
    "#         x = self.dropout2(x)\n",
    "        \n",
    "#         print(x.shape)\n",
    "#         # x = x.view(-1, 64 * 28 * 28)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "#         x = self.relu3(self.bn3(self.fc1(x)))\n",
    "#         x = self.dropout3(x)\n",
    "        \n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class OpenEyesClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OpenEyesClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding='same')\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.1)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding='same')\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.1)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
    "        self.leaky_relu3 = nn.LeakyReLU(0.1)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1152, 128) \n",
    "        self.leaky_relu4 = nn.LeakyReLU(0.1)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu1(self.conv1(x))\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.leaky_relu2(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.leaky_relu3(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.leaky_relu4(self.fc1(x))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        # return self.logsoftmax(x)\n",
    "        return self.sigmoid(x)\n",
    "        # return self.softmax(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "def compute_eer(labels, scores):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_index = np.argmin(np.abs(fnr - fpr))\n",
    "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
    "    thresh = thresholds[eer_index]\n",
    "    return eer, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eer_(labels, scores):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n",
    "    frr = 1 - tpr\n",
    "    abs_diffs = np.abs(fpr - frr)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = (fpr[min_index]+ frr[min_index])/2\n",
    "    \n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels):\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = preds.shape[0]\n",
    "\n",
    "    return correct/total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def evaluate_model(model, criterion, val_loader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    val_labels_list = []\n",
    "    val_preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for it in val_loader:\n",
    "            inputs, labels = it['image'].to(device), it['label'].to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs.permute(1,0), labels.unsqueeze(0))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_labels_list += labels.unsqueeze(0).cpu().detach().numpy().tolist()\n",
    "            val_preds_list += outputs.permute(1,0).cpu().detach().numpy().tolist()\n",
    "\n",
    "        val_eer = compute_eer_(val_labels_list[0], val_preds_list[0])\n",
    "        preds = (torch.tensor(val_labels_list[0]) >= 0.5).float()\n",
    "\n",
    "        val_acc = compute_accuracy(preds, torch.tensor(val_labels_list[0]))\n",
    "\n",
    "    return val_loss, val_acc, val_eer\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, ckpts_path, num_epochs=1, scheduler=None, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    min_eer = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training phase ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        train_labels_list = []\n",
    "        train_preds_list = []\n",
    "        \n",
    "        for it in train_loader:\n",
    "            inputs, labels = it['image'].to(device), it['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs.permute(1,0).shape, labels.unsqueeze(0).shape)\n",
    "            loss = criterion(outputs.permute(1,0), labels.unsqueeze(0))\n",
    "            # eer_train = compute_eer_(labels.unsqueeze(0).cpu().detach().numpy(), outputs.permute(1,0).cpu().detach().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "            train_labels_list += labels.unsqueeze(0).cpu().detach().numpy().tolist()\n",
    "            train_preds_list += outputs.permute(1,0).cpu().detach().numpy().tolist()\n",
    "\n",
    "\n",
    "        train_eer = compute_eer_(train_labels_list[0], train_preds_list[0])\n",
    "        preds = (torch.tensor(train_preds_list[0]) >= 0.5).float()\n",
    "\n",
    "        train_acc = compute_accuracy(preds, torch.tensor(train_labels_list[0]))\n",
    "\n",
    "\n",
    "        print(train_acc, train_eer)\n",
    "\n",
    "        val_loss, val_acc, val_eer = evaluate_model(model, criterion, val_loader, device=device)\n",
    "\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": val_loss, \"train_accuracy\": train_acc, \"train_eer\": train_eer, \\\n",
    "                   \"val_loss\": val_loss, \"val_accuracy\": val_acc, \"val_eer\": val_eer})\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        if val_eer < min_eer or val_eer < 0.01:\n",
    "            min_eer = val_eer\n",
    "            torch.save(model.state_dict(), ckpts_path)\n",
    "            print(f\"Saved model with validation accuracy = {val_acc:.4f} and eer = {val_eer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_dir = '/home/sadevans/space/CloseEyesClassifier/data/clustered_auto_tcne_CHECKED/open'\n",
    "# close_dir = '/home/sadevans/space/CloseEyesClassifier/data/clustered_auto_tcne_CHECKED/close'\n",
    "\n",
    "open_dir = '/home/sadevans/space/CloseEyesClassifier/data/test_images/open'\n",
    "close_dir = '/home/sadevans/space/CloseEyesClassifier/data/test_images/close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_image_paths_and_labels(open_dir, close_dir):\n",
    "    open_images = [os.path.join(open_dir, img) for img in os.listdir(open_dir) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
    "    close_images = [os.path.join(close_dir, img) for img in os.listdir(close_dir) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
    "    images = open_images + close_images\n",
    "    labels = [1] * len(open_images) + [0] * len(close_images)\n",
    "    return images, labels\n",
    "\n",
    "# Перемешать пути изображений и метки классов\n",
    "def shuffle_data(images, labels):\n",
    "    combined = list(zip(images, labels))\n",
    "    random.shuffle(combined)\n",
    "    images[:], labels[:] = zip(*combined)\n",
    "    return images, labels\n",
    "\n",
    "# Разделить данные на обучающую, валидационную и тестовую выборки\n",
    "def split_data(images, labels):\n",
    "    train_images, temp_images, train_labels, temp_labels = train_test_split(images, labels, test_size=0.4, stratify=labels, random_state=42)\n",
    "    val_images, test_images, val_labels, test_labels = train_test_split(temp_images, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n",
    "    return train_images, val_images, test_images, train_labels, val_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# Загрузить данные\n",
    "images, labels = load_image_paths_and_labels(open_dir, close_dir)\n",
    "\n",
    "# Перемешать данные\n",
    "images, labels = shuffle_data(images, labels)\n",
    "\n",
    "# Разделить данные\n",
    "# train_images, val_images, test_images, train_labels, val_labels, test_labels = split_data(images, labels)\n",
    "\n",
    "# print(f\"Train: {len(train_images)} images\")\n",
    "# print(f\"Validation: {len(val_images)} images\")\n",
    "# print(f\"Test: {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ImageTransform, EyeDataset\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# seed = 42\n",
    "\n",
    "# train_dataset = EyeDataset(train_images, train_labels, transform=ImageTransform('train'))\n",
    "# val_dataset = EyeDataset(val_images, val_labels, transform=ImageTransform('val'))\n",
    "test_dataset = EyeDataset(images, labels, transform=ImageTransform('test'))\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from ./open_eyes_classifier.pth\n",
      "5318949003754331940.jpg 0.19110269844532013 0\n",
      "5318949003754331925.jpg 0.9736493229866028 1\n",
      "5318949003754331934.jpg 0.9596593379974365 1\n",
      "5318949003754331939.jpg 0.4209687113761902 0\n",
      "5318949003754331924.jpg 0.9771840572357178 1\n",
      "5318949003754331926.jpg 0.18296509981155396 0\n",
      "5318949003754331941.jpg 0.5017715096473694 0\n",
      "5318949003754331933.jpg 0.0066172038204967976 1\n",
      "5318949003754331936.jpg 0.9771291613578796 1\n",
      "5318949003754331935.jpg 0.7828523516654968 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadevans/space/CloseEyesClassifier/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:824: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  y_true = y_true == pos_label\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(image_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], score, label)\n\u001b[1;32m      8\u001b[0m     scores_list\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m---> 10\u001b[0m test_eer \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_eer_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m preds \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mtensor(scores_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     12\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m compute_accuracy(preds, torch\u001b[38;5;241m.\u001b[39mtensor(labels))\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mcompute_eer_\u001b[0;34m(labels, scores)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_eer_\u001b[39m(labels, scores):\n\u001b[0;32m----> 2\u001b[0m     fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     frr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m tpr\n\u001b[1;32m      4\u001b[0m     abs_diffs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(fpr \u001b[38;5;241m-\u001b[39m frr)\n",
      "File \u001b[0;32m~/space/CloseEyesClassifier/venv/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/space/CloseEyesClassifier/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1095\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    994\u001b[0m     {\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m ):\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1095\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/space/CloseEyesClassifier/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:829\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    827\u001b[0m desc_score_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(y_score, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmergesort\u001b[39m\u001b[38;5;124m\"\u001b[39m)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    828\u001b[0m y_score \u001b[38;5;241m=\u001b[39m y_score[desc_score_indices]\n\u001b[0;32m--> 829\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43my_true\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdesc_score_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     weight \u001b[38;5;241m=\u001b[39m sample_weight[desc_score_indices]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model = OpenEyesClassifier()\n",
    "test_acc = 0.0\n",
    "scores_list = []\n",
    "\n",
    "for image_path, label in zip(images, labels):\n",
    "    score = model.predict(image_path)\n",
    "    print(image_path.split('/')[-1], score, label)\n",
    "    scores_list.append(score)\n",
    "\n",
    "test_eer = compute_eer_(images, scores_list)\n",
    "preds = (torch.tensor(scores_list) > 0.5).float()\n",
    "test_acc = compute_accuracy(preds, torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.4893954396247864, 0.4759499430656433, 0.4773009419441223, 0.4791807532310486, 0.48713383078575134, 0.4869005084037781, 0.4673452377319336, 0.48795607686042786, 0.4844638705253601, 0.4944532513618469, 0.4771513044834137, 0.5044295191764832, 0.504009485244751, 0.4867953360080719, 0.4812712073326111, 0.4881991446018219, 0.4882332980632782, 0.4848036468029022, 0.4834272861480713, 0.47826460003852844, 0.4839402735233307, 0.4751614034175873, 0.48781877756118774, 0.4845618009567261, 0.48473361134529114, 0.4863566756248474, 0.44750821590423584, 0.4907439351081848, 0.47800448536872864, 0.4870065450668335, 0.47488510608673096, 0.49149301648139954]\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "0.40625 0.3765182186234818\n",
      "[0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[0.33174967765808105, 0.1989264041185379, 0.09188774228096008, 0.3854682445526123, 0.553437352180481, 0.6083850264549255, 0.3585776388645172, 0.35463184118270874, 0.3325105905532837, 0.12640617787837982, 0.5627367496490479, 0.48244449496269226, 0.4299934506416321, 0.3643054664134979, 0.5961963534355164, 0.40334972739219666, 0.44589412212371826, 0.272348016500473, 0.40761542320251465, 0.3736898601055145, 0.43644261360168457, 0.28981274366378784, 0.3461391031742096, 0.006476952228695154, 0.21261394023895264, 0.37049445509910583, 0.33698970079421997, 0.3567812144756317, 0.4059474766254425, 0.3943443298339844, 0.11422312259674072, 0.32595357298851013]\n",
      "tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "0.75 0.20833333333333331\n",
      "[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
      "[0.6937605142593384, 0.8369302749633789, 0.9165211319923401, 0.10218139737844467, 0.06358007341623306, 0.3134506344795227, 0.8661211133003235, 0.17809396982192993, 0.29590678215026855, 0.7487822771072388, 0.703373372554779, 0.020850397646427155, 0.8565446734428406, 0.4276587665081024, 0.9497135281562805, 0.23988594114780426, 0.19954301416873932, 0.013956202194094658, 0.6662201285362244, 0.6561607718467712, 0.05662770941853523, 0.7619263529777527, 0.5065041184425354, 0.22613373398780823, 0.0706021711230278, 0.12474113702774048, 0.5732383728027344, 0.018787547945976257, 0.793663501739502, 0.854628324508667, 0.857669472694397, 0.8404795527458191]\n",
      "tensor([1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.])\n",
      "0.90625 0.126984126984127\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "[0.8199045062065125, 0.16890563070774078, 0.00696823513135314, 0.3844105899333954, 0.1818436086177826, 0.3099575638771057, 0.9756356477737427, 0.9406208395957947, 0.9630036354064941, 0.32667598128318787, 0.13916543126106262, 0.03490007296204567, 0.35542377829551697, 0.7779425382614136, 0.05797755345702171, 0.025842107832431793, 0.8690186738967896, 0.5184258222579956, 0.23226793110370636, 0.06701619178056717, 0.8830446600914001, 0.6881607174873352, 0.023325473070144653, 0.22995494306087494, 0.9901700615882874, 0.5255812406539917, 0.500205934047699, 0.6069810390472412, 0.9417162537574768, 0.602498471736908, 0.9330940842628479, 0.5691987872123718]\n",
      "tensor([1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "0.6875 0.18614718614718612\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.006989374291151762, 0.11310924589633942, 0.025457458570599556, 0.7038582563400269, 0.010607226751744747, 0.8623182773590088, 0.3527199625968933, 0.9783855080604553, 0.17358414828777313, 0.005938518326729536, 0.28200289607048035, 0.04583659768104553, 0.9248193502426147, 0.7081944346427917, 0.1334836781024933, 0.6987411379814148, 0.29800209403038025, 0.7952925562858582, 0.05812333524227142, 0.02991143800318241, 0.280710905790329, 0.12331393361091614, 0.20441372692584991, 0.9190158843994141, 0.9395353198051453, 0.8169547915458679, 0.057672884315252304, 0.7765129208564758, 0.04229488596320152, 0.042773839086294174, 0.11672025173902512, 0.013468015938997269]\n",
      "tensor([0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
      "0.96875 0.022727272727272728\n"
     ]
    }
   ],
   "source": [
    "model = OpenEyesClassifier()\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
